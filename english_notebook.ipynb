{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# from utils import preprocess, train_test_split_data\n",
    "from importlib import reload\n",
    "import os\n",
    "import utils\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "utils = reload(utils)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import nltk, nltk.data\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# with open('data/alice_in_wonderland.txt') as f:\n",
    "#     text = f.read()\n",
    "#     print('\\n-----\\n'.join(tokenizer.tokenize(text)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sentencepiece package look for sentence hence linebreak needs to be preserved\n",
    "\n",
    "with open('data/alice_in_wonderland.txt') as f:\n",
    "    text = f.read()\n",
    "    prepro_text = utils.preprocess(text) # preprocessing text\n",
    "    train, test = utils.train_test_split_data(prepro_text, test_size=0.2) # split the data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving the splitted corpus\n",
    "with open('eng_text/train_eng.txt', 'w') as f:\n",
    "    # f.write(str(train))\n",
    "    f.write('\\n'.join(train))\n",
    "\n",
    "with open('eng_text/test_eng.txt', 'w') as f:\n",
    "    # f.write(str(test)) \n",
    "    f.write('\\n'.join(test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comments/ideas\n",
    "* split is not randomized\n",
    "* more steps like lemmatization/stemming can be added in prepro\n",
    "* there is not much preprocessing needs to be done\n",
    "* may be some special characters/ punctuation can be removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Subword Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Granularity: characters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# training model\n",
    "\n",
    "## CHANGE PATH TO SHIFT .model & .vocab in models/sentencepiece\n",
    "\n",
    "# total 72 different types of characters\n",
    "# coverage changes for non-english\n",
    "\n",
    "n_ch = 72\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s1_train \\\n",
    "  --vocab_size=$n_ch \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/train_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s1_train\n",
      "  model_type: BPE\n",
      "  vocab_size: 72\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/train_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1292 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=114468\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1292 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1292\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 4521\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s1_train.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s1_train.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s1_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s1_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# training model - test set\n",
    "\n",
    "## CHANGE PATH TO SHIFT .model & .vocab in models/sentencepiece\n",
    "\n",
    "# total 72 different types of characters\n",
    "# coverage changes for non-english\n",
    "\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s1_test \\\n",
    "  --vocab_size=$n_ch \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/test_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s1_test\n",
      "  model_type: BPE\n",
      "  vocab_size: 72\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/test_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 324 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=28059\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=67\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 324 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 324\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 1709\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=841 min_freq=1\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s1_test.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s1_test.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# segment the text (original) - test set\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s1_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s1_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Granularity: subword units (smaller vocabulary)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "small_vocab = 450\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s2_train \\\n",
    "  --vocab_size=$small_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/train_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s2_train\n",
      "  model_type: BPE\n",
      "  vocab_size: 450\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/train_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1292 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=114468\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1292 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1292\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 4521\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3172 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=619 size=20 all=1257 active=1188 piece=▁c\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=421 size=40 all=1590 active=1521 piece=ar\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=318 size=60 all=1909 active=1840 piece=▁was\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=213 size=80 all=2165 active=2096 piece=▁T\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=153 size=100 all=2458 active=2389 piece=▁an\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=150 min_freq=7\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=116 size=120 all=2631 active=1166 piece=▁D\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=98 size=140 all=2821 active=1356 piece=ge\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=85 size=160 all=3030 active=1565 piece=▁ag\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=76 size=180 all=3165 active=1700 piece=een\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=66 size=200 all=3228 active=1763 piece=▁then\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=65 min_freq=7\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=59 size=220 all=3356 active=1129 piece=▁into\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52 size=240 all=3447 active=1220 piece=▁its\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47 size=260 all=3572 active=1345 piece=▁your\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42 size=280 all=3693 active=1466 piece=▁(\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39 size=300 all=3782 active=1555 piece=▁how\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39 min_freq=6\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36 size=320 all=3865 active=1081 piece=▁such\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34 size=340 all=3931 active=1147 piece=phon\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=360 all=3975 active=1191 piece=ever\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s2_train.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s2_train.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# segment the text (original) -train\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s2_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s2_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# training model - test set\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s2_test \\\n",
    "  --vocab_size=$small_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/test_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s2_test\n",
      "  model_type: BPE\n",
      "  vocab_size: 450\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/test_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 324 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=28059\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=67\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 324 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 324\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 1709\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=841 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=147 size=20 all=1021 active=954 piece=▁of\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=92 size=40 all=1271 active=1204 piece=▁ha\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=70 size=60 all=1495 active=1428 piece=▁she\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=54 size=80 all=1669 active=1602 piece=▁Alice\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38 size=100 all=1796 active=1729 piece=gh\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=38 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=120 all=1958 active=1157 piece=▁wh\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=140 all=2050 active=1249 piece=and\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22 size=160 all=2131 active=1330 piece=▁look\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19 size=180 all=2179 active=1378 piece=▁Queen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17 size=200 all=2238 active=1437 piece=imp\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16 size=220 all=2276 active=1030 piece=▁their\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14 size=240 all=2341 active=1095 piece=ouse\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13 size=260 all=2397 active=1151 piece=ting\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=280 all=2456 active=1210 piece=ning\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=300 all=2536 active=1290 piece=oup\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=320 all=2564 active=1028 piece=▁O\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=340 all=2604 active=1068 piece=▁witness\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=360 all=2635 active=1099 piece=right\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=380 all=2654 active=1118 piece=▁gu\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s2_test.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s2_test.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s2_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s2_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Granularity: subword units (larger vocabulary)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 1500-3000 for best performance\n",
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "large_vocab = 2000\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s3_train \\\n",
    "  --vocab_size=$large_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/train_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s3_train\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/train_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1292 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=114468\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1292 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1292\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 4521\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3172 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=619 size=20 all=1257 active=1188 piece=▁c\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=421 size=40 all=1590 active=1521 piece=ar\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=318 size=60 all=1909 active=1840 piece=▁was\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=213 size=80 all=2165 active=2096 piece=▁T\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=153 size=100 all=2458 active=2389 piece=▁an\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=150 min_freq=7\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=116 size=120 all=2631 active=1166 piece=▁D\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=98 size=140 all=2821 active=1356 piece=ge\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=85 size=160 all=3030 active=1565 piece=▁ag\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=76 size=180 all=3165 active=1700 piece=een\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=66 size=200 all=3228 active=1763 piece=▁then\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=65 min_freq=7\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=59 size=220 all=3356 active=1129 piece=▁into\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52 size=240 all=3447 active=1220 piece=▁its\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47 size=260 all=3572 active=1345 piece=▁your\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42 size=280 all=3693 active=1466 piece=▁(\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39 size=300 all=3782 active=1555 piece=▁how\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39 min_freq=6\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36 size=320 all=3865 active=1081 piece=▁such\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34 size=340 all=3931 active=1147 piece=phon\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=360 all=3975 active=1191 piece=ever\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30 size=380 all=4103 active=1319 piece=▁cat\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28 size=400 all=4211 active=1427 piece=▁ey\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=28 min_freq=6\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27 size=420 all=4239 active=1027 piece=▁March\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=440 all=4290 active=1078 piece=▁won\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24 size=460 all=4353 active=1141 piece=▁every\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22 size=480 all=4393 active=1181 piece=ild\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21 size=500 all=4426 active=1214 piece=de\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=21 min_freq=5\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21 size=520 all=4492 active=1063 piece=▁upon\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20 size=540 all=4526 active=1097 piece=▁rather\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18 size=560 all=4564 active=1135 piece=▁Y\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17 size=580 all=4596 active=1167 piece=▁V\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16 size=600 all=4646 active=1217 piece=ity\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16 min_freq=4\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16 size=620 all=4692 active=1042 piece=▁something\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15 size=640 all=4760 active=1110 piece=▁mean\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14 size=660 all=4815 active=1165 piece=▁TH\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14 size=680 all=4830 active=1180 piece=ertain\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13 size=700 all=4858 active=1208 piece=odo\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=13 min_freq=4\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13 size=720 all=4874 active=1016 piece=▁both\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=740 all=4884 active=1026 piece=ige\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=760 all=4915 active=1057 piece=▁baby\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=780 all=4921 active=1063 piece=dv\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=800 all=4988 active=1130 piece=owed\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=4\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=820 all=5010 active=1020 piece=▁mouth\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=840 all=5001 active=1011 piece=you\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=860 all=5030 active=1040 piece=▁cont\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=880 all=5036 active=1046 piece=▁bottle\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=900 all=5050 active=1060 piece=eer\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9 min_freq=3\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=920 all=5089 active=1039 piece=▁key\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=940 all=5093 active=1043 piece=▁least\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=960 all=5085 active=1035 piece=▁serpent\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=980 all=5099 active=1049 piece=Who\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=1000 all=5156 active=1106 piece=▁che\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8 min_freq=3\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=1020 all=5175 active=1014 piece=▁foll\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=1040 all=5169 active=1008 piece=▁forgot\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=1060 all=5154 active=993 piece=▁direction\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=1080 all=5203 active=1042 piece=iol\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=1100 all=5258 active=1097 piece=ssed\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7 min_freq=3\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=1120 all=5270 active=1009 piece=▁dist\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=1140 all=5270 active=1009 piece=▁puppy\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=1160 all=5259 active=998 piece=▁though\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1180 all=5258 active=997 piece=▁'\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1200 all=5299 active=1038 piece=▁oh\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1220 all=5323 active=1025 piece=time\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1240 all=5344 active=1046 piece=ather\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1260 all=5349 active=1051 piece=▁scre\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1280 all=5347 active=1049 piece=▁inches\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=1300 all=5332 active=1034 piece=▁distance\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1320 all=5345 active=1014 piece=wh\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1340 all=5381 active=1050 piece=▁On\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1360 all=5395 active=1064 piece=▁inv\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1380 all=5402 active=1071 piece=rying\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1400 all=5397 active=1066 piece=▁love\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=5 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1420 all=5395 active=997 piece=▁knock\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1440 all=5386 active=988 piece=▁nearer\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=1460 all=5376 active=978 piece=▁twinkle\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1480 all=5365 active=967 piece=He\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1500 all=5402 active=1004 piece=She\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1520 all=5434 active=1033 piece=tom\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1540 all=5455 active=1054 piece=hand\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1560 all=5471 active=1070 piece=▁ARE\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1580 all=5478 active=1077 piece=▁sig\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1600 all=5491 active=1090 piece=times\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1620 all=5491 active=1000 piece=▁mice\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1640 all=5491 active=1000 piece=ssible\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1660 all=5478 active=987 piece=▁often\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1680 all=5471 active=980 piece=▁corner\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1700 all=5464 active=973 piece=▁sleepy\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1720 all=5448 active=985 piece=▁sounded\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=1740 all=5435 active=972 piece=▁instantly\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1760 all=5435 active=972 piece=Su\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1780 all=5462 active=999 piece=cho\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1800 all=5494 active=1031 piece=▁HE\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1820 all=5502 active=1009 piece=gled\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1840 all=5516 active=1023 piece=▁Eag\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1860 all=5516 active=1023 piece=▁per\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1880 all=5521 active=1028 piece=nging\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1900 all=5517 active=1024 piece=▁days\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=1920 all=5513 active=997 piece=▁pick\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s3_train.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s3_train.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s3_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s3_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# training model - test set\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 1500-3000 for best performance\n",
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s3_test \\\n",
    "  --vocab_size=$large_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../eng_text/test_eng.txt\n",
      "  input_format: \n",
      "  model_prefix: en_s3_test\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../eng_text/test_eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 324 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=28059\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=67\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 324 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 324\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 1709\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=841 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=147 size=20 all=1021 active=954 piece=▁of\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=92 size=40 all=1271 active=1204 piece=▁ha\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=70 size=60 all=1495 active=1428 piece=▁she\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=54 size=80 all=1669 active=1602 piece=▁Alice\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38 size=100 all=1796 active=1729 piece=gh\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=38 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=120 all=1958 active=1157 piece=▁wh\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=140 all=2050 active=1249 piece=and\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22 size=160 all=2131 active=1330 piece=▁look\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19 size=180 all=2179 active=1378 piece=▁Queen\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17 size=200 all=2238 active=1437 piece=imp\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16 size=220 all=2276 active=1030 piece=▁their\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14 size=240 all=2341 active=1095 piece=ouse\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13 size=260 all=2397 active=1151 piece=ting\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12 size=280 all=2456 active=1210 piece=ning\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=300 all=2536 active=1290 piece=oup\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=2\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=320 all=2564 active=1028 piece=▁O\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10 size=340 all=2604 active=1068 piece=▁witness\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9 size=360 all=2635 active=1099 piece=right\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8 size=380 all=2654 active=1118 piece=▁gu\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=400 all=2653 active=1117 piece=ER\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7 size=420 all=2711 active=1053 piece=▁cur\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=440 all=2719 active=1061 piece=ak\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=460 all=2775 active=1117 piece=▁ch\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=480 all=2793 active=1135 piece=▁way\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6 size=500 all=2789 active=1131 piece=▁which\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=520 all=2820 active=1032 piece=ate\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=540 all=2855 active=1067 piece=ever\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=560 all=2869 active=1081 piece=▁knew\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=580 all=2857 active=1069 piece=▁Lizard\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=600 all=2862 active=1074 piece=ead\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=620 all=2905 active=1041 piece=Well\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=640 all=2926 active=1062 piece=▁its\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=660 all=2930 active=1066 piece=▁face\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=680 all=2923 active=1059 piece=▁found\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=700 all=2915 active=1051 piece=▁turning\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=720 all=2909 active=995 piece=cl\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=740 all=2936 active=1022 piece=een\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=760 all=2958 active=1044 piece=ure\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=780 all=2971 active=1057 piece=aper\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=800 all=2993 active=1079 piece=upid\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=820 all=2997 active=1003 piece=There\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=840 all=2997 active=1003 piece=▁dist\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=860 all=2987 active=993 piece=▁talk\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=880 all=2983 active=989 piece=▁slate\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=900 all=2975 active=981 piece=▁simple\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=920 all=2957 active=983 piece=▁executed\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=940 all=2947 active=973 piece=PT\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=960 all=2972 active=998 piece=And\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=980 all=2982 active=1008 piece=eak\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1000 all=3009 active=1035 piece=til\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1020 all=3012 active=1002 piece=Cons\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1040 all=3021 active=1011 piece=orth\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1060 all=3017 active=1007 piece=▁car\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1080 all=3022 active=1012 piece=▁sit\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1100 all=3023 active=1013 piece=other\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1120 all=3017 active=994 piece=▁deny\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1140 all=3008 active=985 piece=▁mark\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1160 all=2996 active=973 piece=▁want\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1180 all=2986 active=963 piece=▁doesn\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1200 all=2977 active=954 piece=▁these\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1220 all=2963 active=987 piece=▁garden\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1240 all=2947 active=971 piece=Consider\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1260 all=2927 active=951 piece=▁through\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=1280 all=2910 active=934 piece=▁important\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1300 all=2891 active=915 piece=.]\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1320 all=2894 active=1004 piece=OI\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1340 all=2900 active=1010 piece=ls\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1360 all=2895 active=1005 piece=ARD\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1380 all=2891 active=1001 piece=ROM\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1400 all=2885 active=995 piece=arm\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1420 all=2887 active=1003 piece=els\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1440 all=2895 active=1011 piece=isk\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1460 all=2902 active=1018 piece=oun\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1480 all=2910 active=1026 piece=vol\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1500 all=2901 active=1017 piece=▁QU\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1520 all=2900 active=999 piece=Coll\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1540 all=2894 active=993 piece=VERY\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1560 all=2893 active=992 piece=cove\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1580 all=2901 active=1000 piece=ieth\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1600 all=2899 active=998 piece=orty\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1620 all=2897 active=998 piece=urry\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1640 all=2884 active=985 piece=▁PRO\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1660 all=2868 active=969 piece=▁emp\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1680 all=2863 active=964 piece=▁our\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1700 all=2851 active=952 piece=Shall\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1720 all=2845 active=995 piece=apers\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1740 all=2846 active=996 piece=essed\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1760 all=2846 active=996 piece=onest\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1780 all=2846 active=996 piece=▁Betw\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1800 all=2828 active=978 piece=▁Turn\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1820 all=2815 active=988 piece=▁days\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1840 all=2798 active=971 piece=▁here\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1860 all=2784 active=957 piece=▁ripp\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1880 all=2770 active=943 piece=▁than\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1900 all=2754 active=927 piece=Behead\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1920 all=2742 active=989 piece=eaking\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_s3_test.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_s3_test.vocab\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s3_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s3_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observation\n",
    "* Character - Almost every single character is segmented\n",
    "* Subword unit (smaller vacob) - The length of segmented subword is longer and many words are also considered as subwords\n",
    "* Subword unit (larger vocab) - Here, subwords are longer. Many words are themself segmented into single subwords. It can also be seen consistently that orignally longer words are broken into two or more segments.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: LM Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Baseline (en_s1 - character level)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Training baseline LM (en_s1)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s1_train.txt' \\\n",
    "    -valid '../../eng_text/en_s1_test.txt' \\\n",
    "    -rnnlm baseline_en_s1 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $n_ch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s1_train.txt\n",
      "valid file: ../../eng_text/en_s1_test.txt\n",
      "class size: 72\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s1\n",
      "Starting training using file ../../eng_text/en_s1_train.txt\n",
      "Vocab size: 70\n",
      "Words in train file: 115760\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.1281    Words/sec: 119015.6   VALID entropy: 3.4054\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.6953    Words/sec: 116162.9   VALID entropy: 3.3093\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.5930    Words/sec: 108518.5   VALID entropy: 3.3068\n",
      "Iter:   3\tAlpha: 0.050000\t   TRAIN entropy: 2.4821    Words/sec: 99745.0   VALID entropy: 3.2160\n",
      "Iter:   4\tAlpha: 0.025000\t   TRAIN entropy: 2.4283    Words/sec: 110235.0   VALID entropy: 3.1463\n",
      "Iter:   5\tAlpha: 0.012500\t   TRAIN entropy: 2.4027    Words/sec: 116410.6   VALID entropy: 3.0978\n",
      "Iter:   6\tAlpha: 0.006250\t   TRAIN entropy: 2.3894    Words/sec: 118344.7   VALID entropy: 3.0674\n",
      "Iter:   7\tAlpha: 0.003125\t   TRAIN entropy: 2.3824    Words/sec: 120176.9   VALID entropy: 3.0518\n",
      "Iter:   8\tAlpha: 0.001563\t   TRAIN entropy: 2.3784    Words/sec: 118001.1   VALID entropy: 3.0455\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Baseline (smaller vocab - en_s2)\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s2_train.txt' \\\n",
    "    -valid '../../eng_text/en_s2_test.txt' \\\n",
    "    -rnnlm baseline_en_s2 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $small_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s2_train.txt\n",
      "valid file: ../../eng_text/en_s2_test.txt\n",
      "class size: 450\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s2\n",
      "Starting training using file ../../eng_text/en_s2_train.txt\n",
      "Vocab size: 442\n",
      "Words in train file: 46742\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 7.5093    Words/sec: 31961.6   VALID entropy: 7.1103\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 6.6438    Words/sec: 30707.2   VALID entropy: 6.5573\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 5.9915    Words/sec: 31639.9   VALID entropy: 6.3010\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 5.5659    Words/sec: 31982.9   VALID entropy: 6.1937\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 5.2843    Words/sec: 31796.5   VALID entropy: 6.1650\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 5.0921    Words/sec: 30927.1   VALID entropy: 6.1694\n",
      "Iter:   6\tAlpha: 0.050000\t   TRAIN entropy: 5.0307    Words/sec: 29972.6   VALID entropy: 6.0552\n",
      "Iter:   7\tAlpha: 0.025000\t   TRAIN entropy: 4.8947    Words/sec: 32831.5   VALID entropy: 5.9892\n",
      "Iter:   8\tAlpha: 0.012500\t   TRAIN entropy: 4.8218    Words/sec: 32511.0   VALID entropy: 5.9569\n",
      "Iter:   9\tAlpha: 0.006250\t   TRAIN entropy: 4.7819    Words/sec: 32811.3   VALID entropy: 5.9344\n",
      "Iter:  10\tAlpha: 0.003125\t   TRAIN entropy: 4.7602    Words/sec: 33134.1   VALID entropy: 5.9203\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Baseline (larger vocab - en_s3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s3_train.txt' \\\n",
    "    -valid '../../eng_text/en_s3_test.txt' \\\n",
    "    -rnnlm baseline_en_s3 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $large_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s3_train.txt\n",
      "valid file: ../../eng_text/en_s3_test.txt\n",
      "class size: 2000\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s3\n",
      "Starting training using file ../../eng_text/en_s3_train.txt\n",
      "Vocab size: 1829\n",
      "Words in train file: 32645\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 8.2916    Words/sec: 6794.7   VALID entropy: 7.1652\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 7.5869    Words/sec: 7154.0   VALID entropy: 6.7712\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 7.1646    Words/sec: 7270.9   VALID entropy: 6.6217\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 6.8658    Words/sec: 7305.7   VALID entropy: 6.4476\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 6.6328    Words/sec: 7231.6   VALID entropy: 6.3239\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 6.4401    Words/sec: 7245.1   VALID entropy: 6.2968\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 6.2748    Words/sec: 7233.1   VALID entropy: 6.2871\n",
      "Iter:   7\tAlpha: 0.050000\t   TRAIN entropy: 6.0499    Words/sec: 7320.8   VALID entropy: 6.1517\n",
      "Iter:   8\tAlpha: 0.025000\t   TRAIN entropy: 5.9230    Words/sec: 7220.5   VALID entropy: 6.0745\n",
      "Iter:   9\tAlpha: 0.012500\t   TRAIN entropy: 5.8583    Words/sec: 7218.8   VALID entropy: 6.0245\n",
      "Iter:  10\tAlpha: 0.006250\t   TRAIN entropy: 5.8263    Words/sec: 7281.0   VALID entropy: 5.9833\n",
      "Iter:  11\tAlpha: 0.003125\t   TRAIN entropy: 5.8116    Words/sec: 7218.2   VALID entropy: 5.9705\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Custom hparam training (character level)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Training baseline LM (en_s1)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s1_train.txt' \\\n",
    "    -valid '../../eng_text/en_s1_test.txt' \\\n",
    "    -rnnlm en_s1 \\\n",
    "      -hidden 70 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 5 \\\n",
    "      -class $n_ch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s1_train.txt\n",
      "valid file: ../../eng_text/en_s1_test.txt\n",
      "class size: 72\n",
      "Hidden layer size: 70\n",
      "BPTT: 5\n",
      "Rand seed: 1\n",
      "rnnlm file: en_s1\n",
      "Starting training using file ../../eng_text/en_s1_train.txt\n",
      "Vocab size: 70\n",
      "Words in train file: 115760\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.1144    Words/sec: 50840.6   VALID entropy: 3.3655\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.6143    Words/sec: 50518.0   VALID entropy: 3.2469\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.4622    Words/sec: 50714.8   VALID entropy: 3.2018\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 2.3755    Words/sec: 50507.6   VALID entropy: 3.1991\n",
      "Iter:   4\tAlpha: 0.050000\t   TRAIN entropy: 2.2564    Words/sec: 50367.4   VALID entropy: 3.0576\n",
      "Iter:   5\tAlpha: 0.025000\t   TRAIN entropy: 2.1936    Words/sec: 50507.4   VALID entropy: 2.9791\n",
      "Iter:   6\tAlpha: 0.012500\t   TRAIN entropy: 2.1612    Words/sec: 50093.7   VALID entropy: 2.9323\n",
      "Iter:   7\tAlpha: 0.006250\t   TRAIN entropy: 2.1440    Words/sec: 50254.4   VALID entropy: 2.9018\n",
      "Iter:   8\tAlpha: 0.003125\t   TRAIN entropy: 2.1343    Words/sec: 50685.5   VALID entropy: 2.8857\n",
      "Iter:   9\tAlpha: 0.001563\t   TRAIN entropy: 2.1286    Words/sec: 50119.6   VALID entropy: 2.8779\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Custom hparam training (smaller vocab - en_s2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Training LM on subword units (smaller vocab - en_s2)\n",
    "\n",
    "#incr hidden -> better pp\n",
    "#incr bptt -> better pp\n",
    "# if class < vocabsize -> worse pp\n",
    "#incr class -> better pp\n",
    "\n",
    "## May be need to simplify the corpus (in preprocessing)\n",
    "\n",
    "      # -bptt-block 1 \\\n",
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s2_train.txt' \\\n",
    "    -valid '../../eng_text/en_s2_test.txt' \\\n",
    "    -rnnlm en_s2 \\\n",
    "      -hidden 100 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 6 \\\n",
    "      -class $small_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s2_train.txt\n",
      "valid file: ../../eng_text/en_s2_test.txt\n",
      "class size: 450\n",
      "Hidden layer size: 100\n",
      "BPTT: 6\n",
      "Rand seed: 1\n",
      "rnnlm file: en_s2\n",
      "Starting training using file ../../eng_text/en_s2_train.txt\n",
      "Vocab size: 442\n",
      "Words in train file: 46742\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 7.5122    Words/sec: 10417.7   VALID entropy: 7.1310\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 6.6173    Words/sec: 9906.2   VALID entropy: 6.5883\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 5.9341    Words/sec: 10865.0   VALID entropy: 6.3313\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 5.4813    Words/sec: 9939.4   VALID entropy: 6.2205\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 5.1746    Words/sec: 10886.0   VALID entropy: 6.1830\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 4.9538    Words/sec: 10795.1   VALID entropy: 6.1716\n",
      "Iter:   6\tAlpha: 0.050000\t   TRAIN entropy: 4.6940    Words/sec: 10727.7   VALID entropy: 6.0343\n",
      "Iter:   7\tAlpha: 0.025000\t   TRAIN entropy: 4.5409    Words/sec: 10745.1   VALID entropy: 5.9346\n",
      "Iter:   8\tAlpha: 0.012500\t   TRAIN entropy: 4.4550    Words/sec: 10869.2   VALID entropy: 5.8847\n",
      "Iter:   9\tAlpha: 0.006250\t   TRAIN entropy: 4.4077    Words/sec: 10763.1   VALID entropy: 5.8619\n",
      "Iter:  10\tAlpha: 0.003125\t   TRAIN entropy: 4.3815    Words/sec: 10809.2   VALID entropy: 5.8546\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Custom hparam training (larger vocab - en_s3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Training LM on subword units (larger vocab - en_s3)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "# How to increase no. of iterations?\n",
    "\n",
    "#incr hidden -> better pp\n",
    "#incr bptt -> worse pp\n",
    "# if class < vocabsize -> worse pp\n",
    "#incr class -> better pp\n",
    "\n",
    "## May be need to simplify the corpus (in preprocessing)\n",
    "\n",
    "      # -bptt-block 1 \\\n",
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s3_train.txt' \\\n",
    "    -valid '../../eng_text/en_s3_test.txt' \\\n",
    "    -rnnlm en_s3 \\\n",
    "      -hidden 140 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $large_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s3_train.txt\n",
      "valid file: ../../eng_text/en_s3_test.txt\n",
      "class size: 2000\n",
      "Hidden layer size: 140\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: en_s3\n",
      "Starting training using file ../../eng_text/en_s3_train.txt\n",
      "Vocab size: 1829\n",
      "Words in train file: 32645\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 8.3227    Words/sec: 1499.2   VALID entropy: 7.2129\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 7.6075    Words/sec: 1702.4   VALID entropy: 6.7901\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 7.1662    Words/sec: 1733.0   VALID entropy: 6.5541\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 6.8485    Words/sec: 1706.6   VALID entropy: 6.4327\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 6.5985    Words/sec: 1708.7   VALID entropy: 6.3583\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 6.3841    Words/sec: 1730.4   VALID entropy: 6.3208\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 6.1901    Words/sec: 1737.4   VALID entropy: 6.3402\n",
      "Iter:   7\tAlpha: 0.050000\t   TRAIN entropy: 6.1122    Words/sec: 1894.4   VALID entropy: 6.2277\n",
      "Iter:   8\tAlpha: 0.025000\t   TRAIN entropy: 5.9600    Words/sec: 1898.6   VALID entropy: 6.1586\n",
      "Iter:   9\tAlpha: 0.012500\t   TRAIN entropy: 5.8820    Words/sec: 1915.6   VALID entropy: 6.0939\n",
      "Iter:  10\tAlpha: 0.006250\t   TRAIN entropy: 5.8461    Words/sec: 1733.2   VALID entropy: 6.0462\n",
      "Iter:  11\tAlpha: 0.003125\t   TRAIN entropy: 5.8292    Words/sec: 1780.2   VALID entropy: 6.0156\n",
      "Iter:  12\tAlpha: 0.001563\t   TRAIN entropy: 5.8193    Words/sec: 1737.1   VALID entropy: 5.9885\n",
      "Iter:  13\tAlpha: 0.000781\t   TRAIN entropy: 5.8124    Words/sec: 1730.6   VALID entropy: 5.9668\n",
      "Iter:  14\tAlpha: 0.000391\t   TRAIN entropy: 5.8071    Words/sec: 1783.9   VALID entropy: 5.9495\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4: Text Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "## 1. Character level granularity\n",
    "\n",
    "!cd models/rnnlm/ \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm  en_s1\\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s1/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "## 2. subword units (smaller vocab - en_s2)\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm en_s2 \\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s2/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "## 3. subword units (larger vocab - en_s3)\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm en_s3 \\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s3/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s1_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s1/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s1/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s2_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s2/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s2/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s3_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s3/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s3/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From inspection of generated text:\n",
    "* With character level granularity, there is a structure of sentence and many generated words are also real. However, the grammar is very bad and some words doesn't exist in english.\n",
    "* With small subword granularity, there are more real words. There is style of quotes to indicate they are dialogues. It still lacks grammar and most sentences doesn't make sense or a complete meaning.\n",
    "* With larger subword vocabulary, more words are real. We can see more dialogues which even indicate they are said by Alice. Overall, more information or richer meaning is delivered here.\n",
    "\n",
    "The quality of generated text improves as we increase the vocabulary size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# decoding all the generated text files\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "        spm_decode \\\n",
    "        --model=en_s1_train.model \\\n",
    "        --input_format=piece \\\n",
    "        < \"../../eng_text/gen_en_s1/${i}.txt\" \\\n",
    "        > \"../../eng_text/gen_en_s1/decod.${i}.txt\"; \\\n",
    "       done\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "!cd models/sentencepiece/ \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "        spm_decode \\\n",
    "        --model=en_s2_train.model \\\n",
    "        --input_format=piece \\\n",
    "        < \"../../eng_text/gen_en_s2/${i}.txt\" \\\n",
    "        > \"../../eng_text/gen_en_s2/decod.${i}.txt\"; \\\n",
    "       done\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "!cd models/sentencepiece/ \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "        spm_decode \\\n",
    "        --model=en_s3_train.model \\\n",
    "        --input_format=piece \\\n",
    "        < \"../../eng_text/gen_en_s3/${i}.txt\" \\\n",
    "        > \"../../eng_text/gen_en_s3/decod.${i}.txt\"; \\\n",
    "       done\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 5 OOV comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# extracting vocabulary\n",
    "train_vocabs = utils.vocab_generator(Path('eng_text/train_eng.txt').open('r').read())\n",
    "test_vocabs = utils.vocab_generator(Path('eng_text/test_eng.txt').open('r').read())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# characters\n",
    "extra_vocab_ch = {}\n",
    "for path in Path('eng_text/gen_en_s1/').rglob('decod*.txt'):\n",
    "    k = int(path.name.split(\".\")[1])\n",
    "    extra_vocab_ch[k] = utils.vocab_generator(Path(path).open('r').read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# smaller vocab\n",
    "extra_vocab_smV = {}\n",
    "for path in Path('eng_text/gen_en_s2/').rglob('decod*.txt'):\n",
    "    k = int(path.name.split(\".\")[1])\n",
    "    extra_vocab_smV[k] = utils.vocab_generator(Path(path).open('r').read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# larger vocab\n",
    "extra_vocab_lrV = {}\n",
    "for path in Path('eng_text/gen_en_s3/').rglob('decod*.txt'):\n",
    "    k = int(path.name.split(\".\")[1])\n",
    "    extra_vocab_lrV[k] = utils.vocab_generator(Path(path).open('r').read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Calc OOV rate (with all generated text at each granularity)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "# character level\n",
    "oov_rate = utils.oov_calculator(\n",
    "    # including all generated text vocab too\n",
    "    train_vocabs + list(set([w for v in extra_vocab_ch.values() for w in v])), \n",
    "    test_vocabs\n",
    ")\n",
    "\n",
    "print(\"OOV rate (character granularity):\", oov_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OOV rate (character granularity): 0.2258326563769293\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# smaller vocabulary subword granularity\n",
    "oov_rate = utils.oov_calculator(\n",
    "    # including all generated text vocab too\n",
    "    train_vocabs + list(set([w for v in extra_vocab_smV.values() for w in v])), \n",
    "    test_vocabs\n",
    ")\n",
    "print(\"OOV rate (smaller vocab granularity):\", oov_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OOV rate (smaller vocab granularity): 0.21121039805036557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# larger vocabulary subword granularity\n",
    "oov_rate = utils.oov_calculator(\n",
    "    # including all generated text vocab too\n",
    "    train_vocabs + list(set([w for v in extra_vocab_lrV.values() for w in v])), \n",
    "    test_vocabs\n",
    ")\n",
    "print(\"OOV rate (larger vocab granularity):\", oov_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OOV rate (larger vocab granularity): 0.23639317627944761\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 OOV rate cal (k times)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# character\n",
    "oov_ch = []\n",
    "for k in range(1,8):\n",
    "    oov_rate = utils.oov_calculator(\n",
    "        train_vocabs + extra_vocab_ch[10**k],\n",
    "        test_vocabs \n",
    "        )\n",
    "    oov_ch.append(oov_rate)\n",
    "\n",
    "print(oov_ch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.32087733549959385, 0.32087733549959385, 0.32087733549959385, 0.3200649878147847, 0.314378554021121, 0.27538586515028435, 0.2258326563769293]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# smaller vocabulary\n",
    "oov_smV = []\n",
    "for k in range(1,8):\n",
    "    oov_rate = utils.oov_calculator(\n",
    "        train_vocabs + extra_vocab_smV[10**k],\n",
    "        test_vocabs \n",
    "        )\n",
    "    oov_smV.append(oov_rate)\n",
    "\n",
    "print(oov_smV)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.32087733549959385, 0.32087733549959385, 0.3200649878147847, 0.31681559707554835, 0.2956945572705118, 0.2502030869212023, 0.21121039805036557]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# larger vocabulary\n",
    "oov_lrV = []\n",
    "for k in range(1,8):\n",
    "    oov_rate = utils.oov_calculator(\n",
    "        train_vocabs + extra_vocab_lrV[10**k],\n",
    "        test_vocabs \n",
    "        )\n",
    "    oov_lrV.append(oov_rate)\n",
    "\n",
    "print(oov_lrV)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.32087733549959385, 0.32087733549959385, 0.3200649878147847, 0.31681559707554835, 0.30869212022745735, 0.28107229894394803, 0.23639317627944761]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# character \n",
    "x = range(1,8)\n",
    "\n",
    "plt.bar(x, oov_ch)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('OOV rate')\n",
    "plt.title(\"OOV: English Corpus (character granularity)\" )\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# smaller vocabulary \n",
    "x = range(1,8)\n",
    "\n",
    "plt.bar(x, oov_smV)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('OOV rate')\n",
    "plt.title(\"OOV: English Corpus (smaller vocabulary granularity)\" )\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# larger vocabulary \n",
    "x = range(1,8)\n",
    "\n",
    "plt.bar(x, oov_lrV)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('OOV rate')\n",
    "plt.title(\"OOV: English Corpus (larger vocabulary granularity)\" )\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "For all level of granularity the oov rate decreases as size of generated corpus (10^k) increases. In a practicle application, I would prefer, a model with smaller subword vocabulary. From our experiment, it gets the lowest OOV rate. Also intuitively, character level granularity doesn't make meaningful words and model long term dependencies.The larger vocabulary subwords becomes very close to actual words. The smaller vocabulary subwords fits better to close the generative gap between characters and whole words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('snlp': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "aaaa3379fdd343e0cf653a09e7e2eeaa049f00680c9206d1a9bbb1ac6ef103f6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}