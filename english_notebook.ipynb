{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from utils import preprocess, train_test_split_data\n",
    "from importlib import reload\n",
    "import os\n",
    "import utils\n",
    "utils = reload(utils)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import nltk, nltk.data\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# with open('data/alice_in_wonderland.txt') as f:\n",
    "#     text = f.read()\n",
    "#     print('\\n-----\\n'.join(tokenizer.tokenize(text)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sentencepiece package look for sentence hence linebreak needs to be preserved\n",
    "\n",
    "with open('data/alice_in_wonderland.txt') as f:\n",
    "    text = f.read()\n",
    "    prepro_text = utils.preprocess(text) # preprocessing text\n",
    "    train, test = utils.train_test_split_data(prepro_text, test_size=0.2) # split the data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving the splitted corpus\n",
    "with open('eng_text/train_eng.txt', 'w') as f:\n",
    "    # f.write(str(train))\n",
    "    f.write('\\n'.join(train))\n",
    "\n",
    "with open('eng_text/test_eng.txt', 'w') as f:\n",
    "    # f.write(str(test)) \n",
    "    f.write('\\n'.join(test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comments/ideas\n",
    "* split is not randomized\n",
    "* more steps like lemmatization/stemming can be added in prepro\n",
    "* there is not much preprocessing needs to be done\n",
    "* may be some special characters/ punctuation can be removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Subword Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Granularity: characters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model\n",
    "\n",
    "## CHANGE PATH TO SHIFT .model & .vocab in models/sentencepiece\n",
    "\n",
    "# total 72 different types of characters\n",
    "# coverage changes for non-english\n",
    "\n",
    "n_ch = 72\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s1_train \\\n",
    "  --vocab_size=$n_ch \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s1_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s1_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model - test set\n",
    "\n",
    "## CHANGE PATH TO SHIFT .model & .vocab in models/sentencepiece\n",
    "\n",
    "# total 72 different types of characters\n",
    "# coverage changes for non-english\n",
    "\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s1_test \\\n",
    "  --vocab_size=$n_ch \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original) - test set\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s1_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s1_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Granularity: subword units (smaller vocabulary)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "small_vocab = 450\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s2_train \\\n",
    "  --vocab_size=$small_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original) -train\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s2_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s2_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model - test set\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s2_test \\\n",
    "  --vocab_size=$small_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s2_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s2_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Granularity: subword units (larger vocabulary)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 1500-3000 for best performance\n",
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "large_vocab = 2000\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/train_eng.txt' \\\n",
    "  --model_prefix=en_s3_train \\\n",
    "  --vocab_size=$large_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s3_train.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/train_eng.txt' \\\n",
    "  > '../../eng_text/en_s3_train.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training model - test set\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 1500-3000 for best performance\n",
    "# training model\n",
    "\n",
    "# coverage changes for non-english\n",
    "# fine-tune vocab_size in range of 100-800 for best performance\n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_train \\\n",
    "  --input='../../eng_text/test_eng.txt' \\\n",
    "  --model_prefix=en_s3_test \\\n",
    "  --vocab_size=$large_vocab \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# segment the text (original)\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_encode \\\n",
    "  --model=en_s3_test.model \\\n",
    "  --output_format=piece \\\n",
    "  < '../../eng_text/test_eng.txt' \\\n",
    "  > '../../eng_text/en_s3_test.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observation\n",
    "* Character - Almost every single character is segmented\n",
    "* Subword unit (smaller vacob) - The length of segmented subword is longer and many words are also considered as subwords\n",
    "* Subword unit (larger vocab) - Here, subwords are longer. Many words are themself segmented into single subwords. It can also be seen consistently that orignally longer words are broken into two or more segments.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: LM Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Baseline (en_s1 - character level)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Training baseline LM (en_s1)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s1_train.txt' \\\n",
    "    -valid '../../eng_text/en_s1_test.txt' \\\n",
    "    -rnnlm baseline_en_s1 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $n_ch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s1_train.txt\n",
      "valid file: ../../eng_text/en_s1_test.txt\n",
      "class size: 72\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s1\n",
      "Starting training using file ../../eng_text/en_s1_train.txt\n",
      "Vocab size: 70\n",
      "Words in train file: 115760\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.1281    Words/sec: 118621.0   VALID entropy: 3.4054\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.6953    Words/sec: 117046.6   VALID entropy: 3.3093\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.5930    Words/sec: 117524.9   VALID entropy: 3.3068\n",
      "Iter:   3\tAlpha: 0.050000\t   TRAIN entropy: 2.4821    Words/sec: 113096.9   VALID entropy: 3.2160\n",
      "Iter:   4\tAlpha: 0.025000\t   TRAIN entropy: 2.4283    Words/sec: 107167.5   VALID entropy: 3.1463\n",
      "Iter:   5\tAlpha: 0.012500\t   TRAIN entropy: 2.4027    Words/sec: 119945.3   VALID entropy: 3.0978\n",
      "Iter:   6\tAlpha: 0.006250\t   TRAIN entropy: 2.3894    Words/sec: 119817.9   VALID entropy: 3.0674\n",
      "Iter:   7\tAlpha: 0.003125\t   TRAIN entropy: 2.3824    Words/sec: 115951.2   VALID entropy: 3.0518\n",
      "Iter:   8\tAlpha: 0.001563\t   TRAIN entropy: 2.3784    Words/sec: 113040.2   VALID entropy: 3.0455\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Baseline (smaller vocab - en_s2)\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s2_train.txt' \\\n",
    "    -valid '../../eng_text/en_s2_test.txt' \\\n",
    "    -rnnlm baseline_en_s2 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $small_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s2_train.txt\n",
      "valid file: ../../eng_text/en_s2_test.txt\n",
      "class size: 450\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s2\n",
      "Starting training using file ../../eng_text/en_s2_train.txt\n",
      "Vocab size: 442\n",
      "Words in train file: 46742\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 7.5093    Words/sec: 30811.5   VALID entropy: 7.1103\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 6.6438    Words/sec: 30687.7   VALID entropy: 6.5573\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 5.9915    Words/sec: 30949.6   VALID entropy: 6.3010\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 5.5659    Words/sec: 30304.5   VALID entropy: 6.1937\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 5.2843    Words/sec: 28543.6   VALID entropy: 6.1650\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 5.0921    Words/sec: 30773.7   VALID entropy: 6.1694\n",
      "Iter:   6\tAlpha: 0.050000\t   TRAIN entropy: 5.0307    Words/sec: 30378.9   VALID entropy: 6.0552\n",
      "Iter:   7\tAlpha: 0.025000\t   TRAIN entropy: 4.8947    Words/sec: 30900.4   VALID entropy: 5.9892\n",
      "Iter:   8\tAlpha: 0.012500\t   TRAIN entropy: 4.8218    Words/sec: 30440.6   VALID entropy: 5.9569\n",
      "Iter:   9\tAlpha: 0.006250\t   TRAIN entropy: 4.7819    Words/sec: 26056.8   VALID entropy: 5.9344\n",
      "Iter:  10\tAlpha: 0.003125\t   TRAIN entropy: 4.7602    Words/sec: 31613.6   VALID entropy: 5.9203\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Baseline (larger vocab - en_s3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s3_train.txt' \\\n",
    "    -valid '../../eng_text/en_s3_test.txt' \\\n",
    "    -rnnlm baseline_en_s3 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $large_vocab"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "debug mode: 2\n",
      "train file: ../../eng_text/en_s3_train.txt\n",
      "valid file: ../../eng_text/en_s3_test.txt\n",
      "class size: 2000\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: baseline_en_s3\n",
      "Starting training using file ../../eng_text/en_s3_train.txt\n",
      "Vocab size: 1829\n",
      "Words in train file: 32645\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 8.2916    Words/sec: 6853.0   VALID entropy: 7.1652\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 7.5869    Words/sec: 6877.2   VALID entropy: 6.7712\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 7.1646    Words/sec: 6753.5   VALID entropy: 6.6217\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 6.8658    Words/sec: 6834.8   VALID entropy: 6.4476\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 6.6328    Words/sec: 6851.3   VALID entropy: 6.3239\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 6.4401    Words/sec: 6609.2   VALID entropy: 6.2968\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 6.2748    Words/sec: 6763.2   VALID entropy: 6.2871\n",
      "Iter:   7\tAlpha: 0.050000\t   TRAIN entropy: 6.0499    Words/sec: 6383.4   VALID entropy: 6.1517\n",
      "Iter:   8\tAlpha: 0.025000\t   TRAIN entropy: 5.9230    Words/sec: 6745.6   VALID entropy: 6.0745\n",
      "Iter:   9\tAlpha: 0.012500\t   TRAIN entropy: 5.8583    Words/sec: 5901.9   VALID entropy: 6.0245\n",
      "Iter:  10\tAlpha: 0.006250\t   TRAIN entropy: 5.8263    Words/sec: 6379.7   VALID entropy: 5.9833\n",
      "Iter:  11\tAlpha: 0.003125\t   TRAIN entropy: 5.8116    Words/sec: 6173.7   VALID entropy: 5.9705\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training baseline LM (en_s1)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s1_train.txt' \\\n",
    "    -valid '../../eng_text/en_s1_test.txt' \\\n",
    "    -rnnlm en_s1 \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class $n_ch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training LM on subword units (smaller vocab - eng_s2)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "# How to increase no. of iterations?\n",
    "\n",
    "#incr hidden -> better pp\n",
    "#incr bptt -> worse pp\n",
    "# if class < vocabsize -> worse pp\n",
    "#incr class -> better pp\n",
    "\n",
    "## May be need to simplify the corpus (in preprocessing)\n",
    "\n",
    "      # -bptt-block 1 \\\n",
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s2_train.txt' \\\n",
    "    -valid '../../eng_text/en_s2_test.txt' \\\n",
    "    -rnnlm en_s2 \\\n",
    "      -hidden 100 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 0 \\\n",
    "      -class $small_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training LM on subword units (larger vocab - eng_s3)\n",
    "\n",
    "# At what setting of rnnlm, we need the get the PP of baseline? \n",
    "# what is the use of class size\n",
    "# How to increase no. of iterations?\n",
    "\n",
    "#incr hidden -> better pp\n",
    "#incr bptt -> worse pp\n",
    "# if class < vocabsize -> worse pp\n",
    "#incr class -> better pp\n",
    "\n",
    "## May be need to simplify the corpus (in preprocessing)\n",
    "\n",
    "      # -bptt-block 1 \\\n",
    "!cd models/rnnlm/ \\\n",
    "    && ../../rnnlm/rnnlm \\\n",
    "    -train '../../eng_text/en_s3_train.txt' \\\n",
    "    -valid '../../eng_text/en_s3_test.txt' \\\n",
    "    -rnnlm en_s3 \\\n",
    "      -hidden 120 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 0 \\\n",
    "      -class $large_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4: Text Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 1. Character level granularity\n",
    "\n",
    "!cd models/rnnlm/ \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm  en_s1\\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s1/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 2. subword units (smaller vocab - en_s2)\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm en_s2 \\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s2/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 3. subword units (larger vocab - en_s3)\n",
    "\n",
    "!cd models/rnnlm \\\n",
    "    && for i in 10 100 1000 10000 100000 1000000 10000000; do \\\n",
    "            ../../rnnlm/rnnlm \\\n",
    "            -rnnlm en_s3 \\\n",
    "            -gen $i \\\n",
    "            -debug 0 \\\n",
    "            >> \"../../eng_text/gen_en_s3/${i}.txt\"; \\\n",
    "       done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s1_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s1/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s1/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s2_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s2/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s2/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# back to original (human readable form) from subword units \n",
    "\n",
    "!cd models/sentencepiece/ \\\n",
    "  &&spm_decode \\\n",
    "  --model=en_s3_train.model \\\n",
    "  --input_format=piece \\\n",
    "  < \"../../eng_text/gen_en_s3/100.txt\" \\\n",
    "  > \"../../eng_text/gen_en_s3/decod_100.txt\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('snlp': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "aaaa3379fdd343e0cf653a09e7e2eeaa049f00680c9206d1a9bbb1ac6ef103f6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}